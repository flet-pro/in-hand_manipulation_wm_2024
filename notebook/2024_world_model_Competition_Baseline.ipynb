{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "229d1070-9828-4b02-8e37-bb8d01af7b75",
   "metadata": {
    "id": "229d1070-9828-4b02-8e37-bb8d01af7b75"
   },
   "source": [
    "# 2024年 世界モデル コンペティション 参考notebook  \n",
    "\n",
    "第8回演習で利用したDreamerに修正を加え，Dreamer v2を用いたベースラインコードになっています．  \n",
    "こちらを動かしていただけば，提出時にエラーが発生しない結果を得ることができます（参考用としてcolabの無料枠で1時間ほどで終わるようにパラメータを変えているため，性能は出ないです）．  \n",
    "\n",
    "**目次**\n",
    "1. [準備](#scrollTo=b986f379-97f5-4449-b4c6-7cc385d1f474)\n",
    "2. [環境の設定](#scrollTo=c7819663-fffc-44e5-842f-779564dd8227)\n",
    "3. [補助機能の実装](#scrollTo=6b9cdd13-ce4a-44b4-a01d-5a19d4e38bae)\n",
    "4. [モデルの実装](#scrollTo=0662612e-701b-41a2-8679-25ad03fef367)\n",
    "5. [学習](#scrollTo=b06c188f-8a87-42e7-9f61-7f385eccc565)\n",
    "6. [モデルの保存](#scrollTo=aa693a51-a4cb-4ad4-be2b-322cbd68443d)\n",
    "7. [学習済みパラメータで評価](#scrollTo=c4b31352-bafa-46ed-8bcc-632a24dfced6)\n",
    "\n",
    "以下良い性能を出すためにできる工夫の例です．  \n",
    "- ハイパーパラメータを調整する．  \n",
    "  - バッチサイズを大きくする．\n",
    "  - 更新回数を増やす（update_freqを小さくする）．\n",
    "  - モデルの次元数を大きくする．  など\n",
    "- Dreamer v2の各モデルのアーキテクチャを変更する．\n",
    "- Dreamer v2以外の学習手法を用いる．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b986f379-97f5-4449-b4c6-7cc385d1f474",
   "metadata": {
    "id": "b986f379-97f5-4449-b4c6-7cc385d1f474"
   },
   "source": [
    "## 1. 準備  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3d2823-ec95-4607-a33f-37bfe410f6b4",
   "metadata": {
    "id": "1c3d2823-ec95-4607-a33f-37bfe410f6b4"
   },
   "source": [
    "必要なライブラリのインストール．各自必要なライブラリがある場合は追加でインストールしてください．  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9qtVaIMAB-",
   "metadata": {
    "id": "5b9qtVaIMAB-"
   },
   "outputs": [],
   "source": [
    "# !pip install gym==0.26.2 gym[atari]==0.26.2 gym[accept-rom-license]==0.26.2 autorom ale-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b7e6f3-880d-4936-a9ea-786d8051c8bf",
   "metadata": {
    "id": "c9b7e6f3-880d-4936-a9ea-786d8051c8bf"
   },
   "source": [
    "### 1.1 ライブラリインポート  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f380bb8a-0a49-43b0-9894-f303670bae41",
   "metadata": {
    "id": "f380bb8a-0a49-43b0-9894-f303670bae41",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MUJOCO_GL=egl\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# import gym\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import ResizeObservation\n",
    "import gymnasium_robotics\n",
    "import torch\n",
    "import torch.distributions as td\n",
    "from torch.distributions import Normal, Categorical, OneHotCategorical, OneHotCategoricalStraightThrough\n",
    "from torch.distributions.kl import kl_divergence\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "%env MUJOCO_GL=egl\n",
    "gym.register_envs(gymnasium_robotics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7819663-fffc-44e5-842f-779564dd8227",
   "metadata": {
    "id": "c7819663-fffc-44e5-842f-779564dd8227"
   },
   "source": [
    "## 2. 環境の設定  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd206f5-6f74-49ae-998f-7598564cdb6e",
   "metadata": {
    "id": "0dd206f5-6f74-49ae-998f-7598564cdb6e"
   },
   "source": [
    "### 2.1 Repeat Action  \n",
    "- こちらで実装している環境を用いてOmnicampus上では評価を行います．  \n",
    "- モデルによって変更する可能性があると想定している部分は以下のとおりです．\n",
    "    - 画像のレンダリングサイズ(ResizeObervationクラスのshape)．\n",
    "    - 同じ行動を繰り返す数（RepeatActionクラスのskip）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c775ec7c-aede-4159-8f9b-3ae5501262c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomManipulateBoxEnv(gym.Env):\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.egg_drop_threshold = 0.05  # 卵が落下したと判断する高さの閾値（メートル単位）\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observation, info = self.env.reset(**kwargs)\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        image = self.env.render() # observation\n",
    "        \n",
    "        return image, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        return self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.env, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "976179d0-2365-4440-bd7c-7e07ae218901",
   "metadata": {
    "id": "976179d0-2365-4440-bd7c-7e07ae218901"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRepeatAction\u001b[39;00m(\u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mWrapper):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    同じ行動を指定された回数自動的に繰り返すラッパー. 観測は最後の行動に対応するものになる\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env, skip\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100_000\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "class RepeatAction(CustomManipulateBoxEnv):\n",
    "    \"\"\"\n",
    "    同じ行動を指定された回数自動的に繰り返すラッパー. 観測は最後の行動に対応するものになる\n",
    "    \"\"\"\n",
    "    def __init__(self, env, skip=4, max_steps=100_000):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.max_steps = max_steps if max_steps else float(\"inf\")  # イテレーションの制限\n",
    "        self.steps = 0  # イテレーション回数のカウント\n",
    "        self.height = env.observation_space.shape[0]\n",
    "        self.width = env.observation_space.shape[1]\n",
    "        self._skip = skip\n",
    "\n",
    "    @property\n",
    "    def observation_space():\n",
    "        img = self.env.render()\n",
    "        return gym.spaces.Box(img)\n",
    "        \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        return obs[0]\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.steps >= self.max_steps:  # 100kに達したら何も返さないようにする\n",
    "            print(\"Reached max iterations.\")\n",
    "            return None\n",
    "\n",
    "        total_reward = 0.0\n",
    "        self.steps += 1\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, _, info = self.env.step(action)\n",
    "            img = self.env.render()\n",
    "\n",
    "            total_reward += reward\n",
    "            if self.steps >= self.max_steps:  # 100kに達したら終端にする\n",
    "                done = True\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return img, total_reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8affc9bc-eabb-49b1-b4ba-24731e960e32",
   "metadata": {
    "id": "8affc9bc-eabb-49b1-b4ba-24731e960e32"
   },
   "outputs": [],
   "source": [
    "def make_env(seed=None, img_size=64, max_steps=100_000):\n",
    "    # env = gym.make(\"ALE/MsPacman-v5\")\n",
    "    env = gym.make('HandManipulateBlockRotateZ-v1', render_mode=\"rgb_array\", max_episode_steps=max_steps)\n",
    "    env = CustomManipulateBoxEnv(env)\n",
    "\n",
    "    # シード固定\n",
    "    # env.seed(seed)\n",
    "    env.reset(seed=seed)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "\n",
    "    # env = ResizeObservation(env, (img_size, img_size))\n",
    "    env = RepeatAction(env=env, skip=4, max_steps=max_steps)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "21443d08-bc90-4eab-adb3-3fcd964d1171",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mmake_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m max_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m\n\u001b[1;32m      3\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHandManipulateBlockRotateZ-v1\u001b[39m\u001b[38;5;124m'\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_episode_steps\u001b[38;5;241m=\u001b[39mmax_steps)\n",
      "Cell \u001b[0;32mIn[45], line 13\u001b[0m, in \u001b[0;36mmake_env\u001b[0;34m(seed, img_size, max_steps)\u001b[0m\n\u001b[1;32m     10\u001b[0m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# env = ResizeObservation(env, (img_size, img_size))\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mRepeatAction\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m env\n",
      "Cell \u001b[0;32mIn[41], line 9\u001b[0m, in \u001b[0;36mRepeatAction.__init__\u001b[0;34m(self, env, skip, max_steps)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_steps \u001b[38;5;241m=\u001b[39m max_steps \u001b[38;5;28;01mif\u001b[39;00m max_steps \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# イテレーションの制限\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# イテレーション回数のカウント\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip \u001b[38;5;241m=\u001b[39m skip\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "env = make_env()\n",
    "max_steps = 100000\n",
    "env = gym.make('HandManipulateBlockRotateZ-v1', render_mode=\"rgb_array\", max_episode_steps=max_steps)\n",
    "env = CustomManipulateBoxEnv(env)\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9cdd13-ce4a-44b4-a01d-5a19d4e38bae",
   "metadata": {
    "id": "6b9cdd13-ce4a-44b4-a01d-5a19d4e38bae"
   },
   "source": [
    "## 3. 補助機能の実装  \n",
    "- モデルを保存する際に利用できるクラス，torchのシード値を固定できる関数です．   \n",
    "- 提出いただくパラメータの保存や読み込みにこちらのクラスを必ず利用する必要はありません  ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "439e4333-d460-4ef2-9aab-5c6f673f8261",
   "metadata": {
    "id": "439e4333-d460-4ef2-9aab-5c6f673f8261"
   },
   "outputs": [],
   "source": [
    "# モデルパラメータをGoogleDriveに保存・後で読み込みするためのヘルパークラス\n",
    "class TrainedModels:\n",
    "    def __init__(self, *models) -> None:\n",
    "        \"\"\"\n",
    "        コンストラクタ．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        models : nn.Module\n",
    "            保存するモデル．複数モデルを渡すことができます．\n",
    "\n",
    "        使用例: trained_models = TraindModels(encoder, rssm, value_model, action_model)\n",
    "        \"\"\"\n",
    "        assert np.all([nn.Module in model.__class__.__bases__ for model in models]), \"Arguments for TrainedModels need to be nn models.\"\n",
    "\n",
    "        self.models = models\n",
    "\n",
    "    def save(self, dir: str) -> None:\n",
    "        \"\"\"\n",
    "        initで渡したモデルのパラメータを保存します．\n",
    "        パラメータのファイル名は01.pt, 02.pt, ... のように連番になっています．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dir : str\n",
    "            パラメータの保存先．\n",
    "        \"\"\"\n",
    "        for i, model in enumerate(self.models):\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(dir, f\"{str(i + 1).zfill(2)}.pt\")\n",
    "            )\n",
    "\n",
    "    def load(self, dir: str, device: str) -> None:\n",
    "        \"\"\"\n",
    "        initで渡したモデルのパラメータを読み込みます．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dir : str\n",
    "            パラメータの保存先．\n",
    "        device : str\n",
    "            モデルをどのデバイス(CPU or GPU)に載せるかの設定．\n",
    "        \"\"\"\n",
    "        for i, model in enumerate(self.models):\n",
    "            model.load_state_dict(\n",
    "                torch.load(\n",
    "                    os.path.join(dir, f\"{str(i + 1).zfill(2)}.pt\"),\n",
    "                    map_location=device\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4880f046-6076-4eea-a0f7-6ef16e875ab7",
   "metadata": {
    "id": "4880f046-6076-4eea-a0f7-6ef16e875ab7"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Pytorch, NumPyのシード値を固定します．これによりモデル学習の再現性を担保できます．\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : int\n",
    "        シード値．\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0662612e-701b-41a2-8679-25ad03fef367",
   "metadata": {
    "id": "0662612e-701b-41a2-8679-25ad03fef367"
   },
   "source": [
    "## 4. モデルの実装  \n",
    "本notebookでは例としてDreamer v2を用いています．  \n",
    "参考: https://arxiv.org/pdf/2010.02193.pdf  \n",
    "\n",
    "**必要なモデル**  \n",
    "- RSSM\n",
    "    - Recurrent model: $h_t = f_{\\phi} (h_{t-1}, z_{t-1}, a_{t-1})$\n",
    "    - Representation model(=posterior): $z_t \\sim q_{\\phi} (z_t | h_t, x_t)$\n",
    "    - Transition predictor(=prior): $\\hat{z}_t \\sim p_{\\phi} (\\hat{z}_t | h_t)$\n",
    "- Image predictor(=Decoder): $\\hat{x}_t \\sim p_{\\phi} (\\hat{x}_t | h_t, z_t)$\n",
    "- Reward predictor(=RewardModel): $\\hat{r}_t \\sim p_{\\phi} (\\hat{r}_t | h_t, z_t)$\n",
    "- Discount predictor(=DiscountModel): $\\hat{\\gamma}_t \\sim p_{\\phi} (\\hat{\\gamma}_t | h_t, z_t)$\n",
    "- (Encoder): x_tを入力するときには，一度ベクトルに変換する．\n",
    "- Actor: $\\hat{a}_t \\sim p_{\\psi}(\\hat{a}_t | \\hat{z}_t)$\n",
    "- Critic: $v_{\\xi}(\\hat{z}_t) \\approx E_{p_{\\phi}, p_{\\psi}} [\\sum_{{\\tau} \\geq t} \\hat{\\gamma}^{\\tau- t} \\hat{r}_{\\tau}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ddc0b4e-957a-4bb8-add1-48c5d61add23",
   "metadata": {
    "id": "4ddc0b4e-957a-4bb8-add1-48c5d61add23"
   },
   "outputs": [],
   "source": [
    "class RSSM(nn.Module):\n",
    "    def __init__(self, mlp_hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int, actino_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_hidden_dim = rnn_hidden_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Recurrent model\n",
    "        # h_t = f(h_t-1, z_t-1, a_t-1)\n",
    "        self.transition_hidden = nn.Linear(state_dim * num_classes + action_dim, mlp_hidden_dim)\n",
    "        self.transition = nn.GRUCell(mlp_hidden_dim, rnn_hidden_dim)\n",
    "\n",
    "        # transition predictor\n",
    "        self.prior_hidden = nn.Linear(rnn_hidden_dim, mlp_hidden_dim)\n",
    "        self.prior_logits = nn.Linear(mlp_hidden_dim, state_dim * num_classes)\n",
    "\n",
    "        # representation model\n",
    "        self.posterior_hidden = nn.Linear(rnn_hidden_dim + 1536, mlp_hidden_dim)\n",
    "        self.posterior_logits = nn.Linear(mlp_hidden_dim, state_dim * num_classes)\n",
    "\n",
    "    def recurrent(self, state: torch.Tensor, action: torch.Tensor, rnn_hidden: torch.Tensor):\n",
    "        # recullent model: h_t = f(h_t-1, z_t-1, a_t-1)を計算する\n",
    "        hidden = F.elu(self.transition_hidden(torch.cat([state, action], dim=1)))\n",
    "        rnn_hidden = self.transition(hidden, rnn_hidden)\n",
    "\n",
    "        return rnn_hidden  # h_t\n",
    "\n",
    "    def get_prior(self, rnn_hidden: torch.Tensor, detach=False):\n",
    "        # transition predictor: \\hat{z}_t ~ p(z\\hat{z}_t | h_t)\n",
    "        hidden = F.elu(self.prior_hidden(rnn_hidden))\n",
    "        logits = self.prior_logits(hidden)\n",
    "        logits = logits.reshape(logits.shape[0], self.state_dim, self.num_classes)\n",
    "\n",
    "        prior_dist = td.Independent(OneHotCategoricalStraightThrough(logits=logits), 1)\n",
    "        if detach:\n",
    "            detach_prior = td.Independent(OneHotCategoricalStraightThrough(logits=logits.detach()), 1)\n",
    "            return prior_dist, detach_prior  # p(z\\hat{z}_t | h_t)\n",
    "        return prior_dist\n",
    "\n",
    "    def get_posterior(self, rnn_hidden: torch.Tensor, embedded_obs: torch.Tensor, detach=False):\n",
    "        # representation predictor: z_t ~ q(z_t | h_t, o_t)\n",
    "        hidden = F.elu(self.posterior_hidden(torch.cat([rnn_hidden, embedded_obs], dim=1)))\n",
    "        logits = self.posterior_logits(hidden)\n",
    "        logits = logits.reshape(logits.shape[0], self.state_dim, self.num_classes)\n",
    "\n",
    "        posterior_dist = td.Independent(OneHotCategoricalStraightThrough(logits=logits), 1)\n",
    "        if detach:\n",
    "            detach_posterior = td.Independent(OneHotCategoricalStraightThrough(logits=logits.detach()), 1)\n",
    "            return posterior_dist, detach_posterior  # q(z_t | h_t, o_t)\n",
    "        return posterior_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccea4eb0-c866-41bf-96bc-32b82664de94",
   "metadata": {
    "id": "ccea4eb0-c866-41bf-96bc-32b82664de94"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 48, kernel_size=4, stride=2)\n",
    "        self.conv2 = nn.Conv2d(48, 96, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(96, 192, kernel_size=4, stride=2)\n",
    "        self.conv4 = nn.Conv2d(192, 384, kernel_size=4, stride=2)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor):\n",
    "        \"\"\"\n",
    "        観測画像をベクトルに埋め込むためのEncoder．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        obs : torch.Tensor (B, C, H, W)\n",
    "            入力となる観測画像．\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        embedded_obs : torch.Tensor (B, D)\n",
    "            観測画像をベクトルに変換したもの．Dは入力画像の幅と高さに依存して変わる．\n",
    "            入力が(B, 3, 64, 64)の場合，出力は(B, 1536)になる．\n",
    "        \"\"\"\n",
    "        hidden = F.elu(self.conv1(obs))\n",
    "        hidden = F.elu(self.conv2(hidden))\n",
    "        hidden = F.elu(self.conv3(hidden))\n",
    "        embedded_obs = self.conv4(hidden).reshape(hidden.size(0), -1)\n",
    "\n",
    "        return embedded_obs  # x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16a0dbc5-2eb2-4237-b1d3-63e23b8b190e",
   "metadata": {
    "id": "16a0dbc5-2eb2-4237-b1d3-63e23b8b190e"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(state_dim*num_classes + rnn_hidden_dim, 1536)\n",
    "        self.dc1 = nn.ConvTranspose2d(1536, 192, kernel_size=5, stride=2)\n",
    "        self.dc2 = nn.ConvTranspose2d(192, 96, kernel_size=5, stride=2)\n",
    "        self.dc3 = nn.ConvTranspose2d(96, 48, kernel_size=6, stride=2)\n",
    "        self.dc4 = nn.ConvTranspose2d(48, 1, kernel_size=6, stride=2)\n",
    "\n",
    "\n",
    "    def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor):\n",
    "        \"\"\"\n",
    "        決定論的状態と，確率的状態を入力として，観測画像を復元するDecoder．\n",
    "        出力は多次元正規分布の平均値をとる．\n",
    "\n",
    "        Paremters\n",
    "        ---------\n",
    "        state : torch.Tensor (B, state_dim * num_classes)\n",
    "            確率的状態．\n",
    "        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n",
    "            決定論的状態．\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        obs_dist : torch.distribution.Independent\n",
    "            観測画像を再構成するための多次元正規分布．\n",
    "        \"\"\"\n",
    "        hidden = self.fc(torch.cat([state, rnn_hidden], dim=1))\n",
    "        hidden = hidden.view(hidden.size(0), 1536, 1, 1)\n",
    "        hidden = F.elu(self.dc1(hidden))\n",
    "        hidden = F.elu(self.dc2(hidden))\n",
    "        hidden = F.elu(self.dc3(hidden))\n",
    "        mean = self.dc4(hidden)\n",
    "\n",
    "        obs_dist = td.Independent(td.Normal(mean, 1), 3)\n",
    "        return obs_dist  # p(\\hat{x}_t | h_t, z_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ed6a2fb-5c32-4d7f-8d6d-98c66dd3e6da",
   "metadata": {
    "id": "4ed6a2fb-5c32-4d7f-8d6d-98c66dd3e6da"
   },
   "outputs": [],
   "source": [
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim*num_classes + rnn_hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor):\n",
    "        \"\"\"\n",
    "        決定論的状態と，確率的状態を入力として，報酬を予測するモデル．\n",
    "        出力は正規分布の平均値をとる．\n",
    "\n",
    "        Paremters\n",
    "        ---------\n",
    "        state : torch.Tensor (B, state_dim * num_classes)\n",
    "            確率的状態．\n",
    "        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n",
    "            決定論的状態．\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        reward_dist : torch.distribution.Independent\n",
    "            報酬を予測するための正規分布．\n",
    "        \"\"\"\n",
    "        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n",
    "        hidden = F.elu(self.fc2(hidden))\n",
    "        hidden = F.elu(self.fc3(hidden))\n",
    "        mean = self.fc4(hidden)\n",
    "\n",
    "        reward_dist = td.Independent(td.Normal(mean, 1),  1)\n",
    "        return reward_dist  # p(\\hat{r}_t | h_t, z_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5aabc5c5-ab7e-41c7-aadd-b5d66aa91a21",
   "metadata": {
    "id": "5aabc5c5-ab7e-41c7-aadd-b5d66aa91a21"
   },
   "outputs": [],
   "source": [
    "class DiscountModel(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim*num_classes + rnn_hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor):\n",
    "        \"\"\"\n",
    "        決定論的状態と，確率的状態を入力として，現在の状態がエピソード終端かどうか判別するモデル．\n",
    "        出力はベルヌーイ分布の平均値をとる．\n",
    "\n",
    "        Paremters\n",
    "        ---------\n",
    "        state : torch.Tensor (B, state_dim * num_classes)\n",
    "            確率的状態．\n",
    "        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n",
    "            決定論的状態．\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        discount_dist : torch.distribution.Independent\n",
    "            状態が終端かどうかを予測するためのベルヌーイ分布．\n",
    "        \"\"\"\n",
    "        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n",
    "        hidden = F.elu(self.fc2(hidden))\n",
    "        hidden = F.elu(self.fc3(hidden))\n",
    "        mean= self.fc4(hidden)\n",
    "\n",
    "        discount_dist = td.Independent(td.Bernoulli(logits=mean),  1)\n",
    "        return discount_dist  # p(\\hat{\\gamma}_t | h_t, z_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23aeb172-72bc-4a34-99fe-090e6971f426",
   "metadata": {
    "id": "23aeb172-72bc-4a34-99fe-090e6971f426"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, action_dim: int, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(state_dim * num_classes + rnn_hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state: torch.tensor, rnn_hidden: torch.Tensor, eval: bool = False):\n",
    "        \"\"\"\n",
    "        確率的状態を入力として，criticで推定される価値が最大となる行動を出力する．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : torch.Tensor (B, state_dim * num_classes)\n",
    "            確率的状態．\n",
    "        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n",
    "            決定論的状態．\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : torch.Tensor (B, 1)\n",
    "            行動．\n",
    "        action_log_prob : torch.Tensor(B, 1)\n",
    "            予測した行動をとる確率の対数．\n",
    "        action_entropy : torch.Tensor(B, 1)\n",
    "            予測した確率分布のエントロピー．エントロピー正則化に使用．\n",
    "        \"\"\"\n",
    "        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n",
    "        hidden = F.elu(self.fc2(hidden))\n",
    "        hidden = F.elu(self.fc3(hidden))\n",
    "        hidden = F.elu(self.fc4(hidden))\n",
    "        logits = self.out(hidden)\n",
    "\n",
    "        if eval:\n",
    "            action = torch.argmax(logits, dim=1)\n",
    "            action = F.one_hot(action, logits.shape[1])\n",
    "            return action, None, None\n",
    "\n",
    "        action_dist = OneHotCategorical(logits=logits)  # 行動をサンプリングする分布: p_{\\psi} (\\hat{a}_t | \\hat{z}_t)\n",
    "        action = action_dist.sample()  # 行動: \\hat{a}_t\n",
    "\n",
    "        # Straight-Throught Estimatorで勾配を通す．\n",
    "        action = action + action_dist.probs - action_dist.probs.detach()\n",
    "\n",
    "        action_log_prob = action_dist.log_prob(torch.round(action.detach()))\n",
    "        action_entropy = action_dist.entropy()\n",
    "\n",
    "        return action, action_log_prob, action_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7c131f9-1c2a-4549-b178-3e0070e7f5d0",
   "metadata": {
    "id": "f7c131f9-1c2a-4549-b178-3e0070e7f5d0"
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(state_dim * num_classes + rnn_hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state: torch.tensor, rnn_hidden: torch.Tensor):\n",
    "        \"\"\"\n",
    "        確率的状態を入力として，価値関数(lambda target)の値を予測する．．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : torch.Tensor (B, state_dim * num_classes)\n",
    "            確率的状態．\n",
    "        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n",
    "            決定論的状態．\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        value : torch.Tensor (B, 1)\n",
    "            入力された状態に対する状態価値関数の予測値．\n",
    "        \"\"\"\n",
    "        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n",
    "        hidden = F.elu(self.fc2(hidden))\n",
    "        hidden = F.elu(self.fc3(hidden))\n",
    "        hidden = F.elu(self.fc4(hidden))\n",
    "        mean = self.out(hidden)\n",
    "\n",
    "        return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04479090-f3e5-4283-9680-327c5dd1652d",
   "metadata": {
    "id": "04479090-f3e5-4283-9680-327c5dd1652d"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\"\n",
    "    RNNを用いて訓練するのに適したリプレイバッファ\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity, observation_shape, action_dim):\n",
    "        self.capacity = capacity\n",
    "\n",
    "        self.observations = np.zeros((capacity, *observation_shape), dtype=np.float32)\n",
    "        self.actions = np.zeros((capacity, action_dim), dtype=np.uint8)\n",
    "        self.rewards = np.zeros((capacity, 1), dtype=np.float32)\n",
    "        self.done = np.zeros((capacity, 1), dtype=bool)\n",
    "\n",
    "        self.index = 0\n",
    "        self.is_filled = False\n",
    "\n",
    "    def push(self, observation, action, reward, done):\n",
    "        \"\"\"\n",
    "        リプレイバッファに経験を追加する\n",
    "        \"\"\"\n",
    "        self.observations[self.index] = observation\n",
    "        self.actions[self.index] = action\n",
    "        self.rewards[self.index] = reward\n",
    "        self.done[self.index] = done\n",
    "\n",
    "        # indexは巡回し, 最も古い経験を上書きする\n",
    "        if self.index == self.capacity - 1:\n",
    "            self.is_filled = True\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, chunk_length):\n",
    "        \"\"\"\n",
    "        経験をリプレイバッファからサンプルします. （ほぼ）一様なサンプルです\n",
    "        結果として返ってくるのは観測(画像), 行動, 報酬, 終了シグナルについての(batch_size, chunk_length, 各要素の次元)の配列です\n",
    "        各バッチは連続した経験になっています\n",
    "        注意: chunk_lengthをあまり大きな値にすると問題が発生する場合があります\n",
    "        \"\"\"\n",
    "        episode_borders = np.where(self.done)[0]\n",
    "        sampled_indexes = []\n",
    "        for _ in range(batch_size):\n",
    "            cross_border = True\n",
    "            while cross_border:\n",
    "                initial_index = np.random.randint(len(self) - chunk_length + 1)\n",
    "                final_index = initial_index + chunk_length - 1\n",
    "                cross_border = np.logical_and(initial_index <= episode_borders,\n",
    "                                              episode_borders < final_index).any()#論理積\n",
    "            sampled_indexes += list(range(initial_index, final_index + 1))\n",
    "\n",
    "        sampled_observations = self.observations[sampled_indexes].reshape(\n",
    "            batch_size, chunk_length, *self.observations.shape[1:])\n",
    "        sampled_actions = self.actions[sampled_indexes].reshape(\n",
    "            batch_size, chunk_length, self.actions.shape[1])\n",
    "        sampled_rewards = self.rewards[sampled_indexes].reshape(\n",
    "            batch_size, chunk_length, 1)\n",
    "        sampled_done = self.done[sampled_indexes].reshape(\n",
    "            batch_size, chunk_length, 1)\n",
    "        return sampled_observations, sampled_actions, sampled_rewards, sampled_done\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.capacity if self.is_filled else self.index\n",
    "\n",
    "    def save(self, dir: str):\n",
    "        np.save(f\"{dir}/observations\", self.observations)\n",
    "        np.save(f\"{dir}/actions\", self.actions)\n",
    "        np.save(f\"{dir}/rewards\", self.rewards)\n",
    "        np.save(f\"{dir}/done\", self.done)\n",
    "\n",
    "    def load(self, dir: str):\n",
    "        self.observations = np.load(f\"{dir}/observations.npy\")\n",
    "        self.actions = np.load(f\"{dir}/actions.npy\")\n",
    "        self.rewards = np.load(f\"{dir}/rewards.npy\")\n",
    "        self.done = np.load(f\"{dir}/done.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21a4bc91-83d0-422c-b838-7783985f0473",
   "metadata": {
    "id": "21a4bc91-83d0-422c-b838-7783985f0473"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_lambda_target\u001b[39m(rewards: \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mTensor, discounts: torch\u001b[38;5;241m.\u001b[39mTensor, values: torch\u001b[38;5;241m.\u001b[39mTensor, lambda_: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    lambda targetを計算する関数．\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m        lambda targetの値．\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     V_lambda \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(rewards)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def calculate_lambda_target(rewards: torch.Tensor, discounts: torch.Tensor, values: torch.Tensor, lambda_: float):\n",
    "    \"\"\"\n",
    "    lambda targetを計算する関数．\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    rewards : torch.Tensor (imagination_horizon, D)\n",
    "        報酬．1次元目が時刻tを表しており，2次元目は自由な次元数にでき，想像の軌道を作成するときに入力されるサンプルindexと考える．\n",
    "    discounts : torch.Tensor (imagination_horizon, D)\n",
    "        割引率．gammaそのままを利用するのではなく，DiscountModelの出力をかけて利用する．\n",
    "    values : torch.Tensor (imagination_horizon, D)\n",
    "        状態価値関数．criticで予測された値を利用するが，Dreamer v2ではtarget networkで計算する．\n",
    "    lambda_ : float\n",
    "        lambda targetのハイパラ．\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    V_lambda : torch.Tensor (imagination_horizon, D)\n",
    "        lambda targetの値．\n",
    "    \"\"\"\n",
    "    V_lambda = torch.zeros_like(rewards)\n",
    "\n",
    "    for t in reversed(range(rewards.shape[0])):\n",
    "        if t == rewards.shape[0] - 1:\n",
    "            V_lambda[t] = rewards[t] + discounts[t] * values[t]  # t=Hの場合（式4の下の条件）\n",
    "        else:\n",
    "            V_lambda[t] = rewards[t] + discounts[t] * ((1-lambda_) * values[t+1] + lambda_ * V_lambda[t+1])\n",
    "\n",
    "    return V_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8c1ac46-0d4f-4edc-a696-24fc2fc22c27",
   "metadata": {
    "id": "c8c1ac46-0d4f-4edc-a696-24fc2fc22c27"
   },
   "outputs": [],
   "source": [
    "def preprocess_obs(obs):\n",
    "    \"\"\"\n",
    "    画像の変換. [0, 255] -> [0, 1]\n",
    "    \"\"\"\n",
    "    height, width = obs.shape[0], obs.shape[1]\n",
    "    obs = Image.fromarray(obs)\n",
    "    obs = obs.convert(\"L\")\n",
    "    obs = np.array(obs).reshape(height, width, 1)\n",
    "    obs = obs.astype(np.float32)\n",
    "    normalized_obs = obs / 255.0 - 0.5\n",
    "    return normalized_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1e6b57b-efa4-4ecc-a37c-94f82942c03b",
   "metadata": {
    "id": "d1e6b57b-efa4-4ecc-a37c-94f82942c03b"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    ActionModelに基づき行動を決定する. そのためにRSSMを用いて状態表現をリアルタイムで推論して維持するクラス\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, rssm, action_model):\n",
    "        self.encoder = encoder\n",
    "        self.rssm = rssm\n",
    "        self.action_model = action_model\n",
    "\n",
    "        self.device = next(self.action_model.parameters()).device\n",
    "        self.rnn_hidden = torch.zeros(1, rssm.rnn_hidden_dim, device=self.device)\n",
    "\n",
    "    def __call__(self, obs, eval=False):\n",
    "        # preprocessを適用, PyTorchのためにChannel-Firstに変換\n",
    "        obs = preprocess_obs(obs)\n",
    "        obs = torch.as_tensor(obs, device=self.device)\n",
    "        obs = obs.transpose(1, 2).transpose(0, 1).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 観測を低次元の表現に変換し, posteriorからのサンプルをActionModelに入力して行動を決定する\n",
    "            embedded_obs = self.encoder(obs)\n",
    "            state_posterior = self.rssm.get_posterior(self.rnn_hidden, embedded_obs)\n",
    "            state = state_posterior.sample().flatten(1)\n",
    "            action, _, _  = self.action_model(state, self.rnn_hidden, eval=eval)\n",
    "\n",
    "            # 次のステップのためにRNNの隠れ状態を更新しておく\n",
    "            self.rnn_hidden = self.rssm.recurrent(state, action, self.rnn_hidden)\n",
    "\n",
    "        return action.squeeze().cpu().numpy()\n",
    "\n",
    "    #RNNの隠れ状態をリセット\n",
    "    def reset(self):\n",
    "        self.rnn_hidden = torch.zeros(1, self.rssm.rnn_hidden_dim, device=self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06c188f-8a87-42e7-9f61-7f385eccc565",
   "metadata": {
    "id": "b06c188f-8a87-42e7-9f61-7f385eccc565"
   },
   "source": [
    "## 5. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48a14cd1-dc80-4f22-a199-131fa267e14c",
   "metadata": {
    "id": "48a14cd1-dc80-4f22-a199-131fa267e14c"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, **kwargs):\n",
    "        # コメントアウトされている値は，元実装のハイパーパラメータの値\n",
    "        # data settings\n",
    "        self.buffer_size = 100_000  # バッファにためるデータの上限\n",
    "        self.batch_size = 50  # 50  # 学習時のバッチサイズ\n",
    "        self.seq_length = 50  # 各バッチの系列長\n",
    "        self.imagination_horizon = 15  # 15  # 想像上の軌道の系列長\n",
    "\n",
    "        # model dimensions\n",
    "        self.state_dim = 32  # 32  # 確率的な状態の次元数\n",
    "        self.num_classes = 32  # 32  # 確率的な状態のクラス数（離散表現のため）\n",
    "        self.rnn_hidden_dim = 600  # 600  # 決定論的な状態の次元数\n",
    "        self.mlp_hidden_dim = 400  # 400  # MLPの隠れ層の次元数\n",
    "\n",
    "        # learning params\n",
    "        self.model_lr = 2e-4  # world model(transition / prior / posterior / discount / image predictor)の学習率\n",
    "        self.actor_lr = 4e-5  # actorの学習率\n",
    "        self.critic_lr = 1e-4  # criticの学習率\n",
    "        self.epsilon = 1e-5  # optimizerのepsilonの値\n",
    "        self.weight_decay = 1e-6  # weight decayの係数\n",
    "        self.gradient_clipping = 100  # 勾配クリッピング\n",
    "        self.kl_scale = 0.1  # kl lossのスケーリング係数\n",
    "        self.kl_balance = 0.8  # kl balancingの係数(fix posterior)\n",
    "        self.actor_entropy_scale = 1e-3  # entropy正則化のスケーリング係数\n",
    "        self.slow_critic_update = 100  # target critic networkの更新頻度\n",
    "        self.reward_loss_scale = 1.0  # reward lossのスケーリング係数\n",
    "        self.discount_loss_scale = 1.0  # discount lossのスケーリング係数\n",
    "        self.update_freq = 80  # 4\n",
    "\n",
    "        # lambda return params\n",
    "        self.discount = 0.995  # 割引率\n",
    "        self.lambda_ = 0.95  # lambda returnのパラメータ\n",
    "\n",
    "        # learning period settings\n",
    "        self.seed_iter = 5_000  # 事前にランダム行動で探索する回数\n",
    "        self.eval_freq = 5  # 評価頻度（エピソード）\n",
    "        self.eval_episodes = 5  # 評価に用いるエピソード数\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d64e973f-6c7d-48ea-8ff5-aca31361067f",
   "metadata": {
    "id": "d64e973f-6c7d-48ea-8ff5-aca31361067f"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m NUM_ITER \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100_000\u001b[39m  \u001b[38;5;66;03m# 環境とのインタラクション回数の制限 ※変更しないでください\u001b[39;00m\n\u001b[1;32m      4\u001b[0m set_seed(seed)\n\u001b[0;32m----> 5\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mmake_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_ITER\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m eval_env \u001b[38;5;241m=\u001b[39m make_env(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1234\u001b[39m, max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# omnicampus上の環境と同じシード値で評価環境を作成\u001b[39;00m\n\u001b[1;32m      7\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[34], line 12\u001b[0m, in \u001b[0;36mmake_env\u001b[0;34m(seed, img_size, max_steps)\u001b[0m\n\u001b[1;32m      9\u001b[0m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m     10\u001b[0m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m---> 12\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mResizeObservation\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m env \u001b[38;5;241m=\u001b[39m RepeatAction(env\u001b[38;5;241m=\u001b[39menv, skip\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, max_steps\u001b[38;5;241m=\u001b[39mmax_steps)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m env\n",
      "File \u001b[0;32m~/venv/work/lib/python3.10/site-packages/gymnasium/wrappers/transform_observation.py:367\u001b[0m, in \u001b[0;36mResizeObservation.__init__\u001b[0;34m(self, env, shape)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env: gym\u001b[38;5;241m.\u001b[39mEnv[ObsType, ActType], shape: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]):\n\u001b[1;32m    361\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Constructor that requires an image environment observation space with a shape.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m        env: The environment to wrap\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m        shape: The resized observation shape\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(env\u001b[38;5;241m.\u001b[39mobservation_space, spaces\u001b[38;5;241m.\u001b[39mBox)\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m}\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mlow \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(\n\u001b[1;32m    370\u001b[0m         env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mhigh \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[1;32m    371\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# モデル等の初期化\n",
    "seed = 0\n",
    "NUM_ITER = 100_000  # 環境とのインタラクション回数の制限 ※変更しないでください\n",
    "set_seed(seed)\n",
    "env = make_env(max_steps=NUM_ITER)\n",
    "eval_env = make_env(seed=1234, max_steps=None)  # omnicampus上の環境と同じシード値で評価環境を作成\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "action_dim = env.action_space.n\n",
    "# リプレイバッファ\n",
    "replay_buffer = ReplayBuffer(\n",
    "    capacity=cfg.buffer_size,\n",
    "    observation_shape=(64, 64, 1),\n",
    "    action_dim=env.action_space.n\n",
    ")\n",
    "\n",
    "# モデル\n",
    "rssm = RSSM(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes, action_dim).to(device)\n",
    "encoder = Encoder().to(device)\n",
    "decoder = Decoder(cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
    "reward_model =  RewardModel(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
    "discount_model = DiscountModel(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
    "actor = Actor(action_dim, cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
    "critic = Critic(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
    "target_critic = Critic(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
    "target_critic.load_state_dict(critic.state_dict())\n",
    "\n",
    "trained_models = TrainedModels(\n",
    "    rssm,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    reward_model,\n",
    "    discount_model,\n",
    "    actor,\n",
    "    critic\n",
    ")\n",
    "\n",
    "# optimizer\n",
    "wm_params = list(rssm.parameters())         + \\\n",
    "            list(encoder.parameters())      + \\\n",
    "            list(decoder.parameters())      + \\\n",
    "            list(reward_model.parameters()) + \\\n",
    "            list(discount_model.parameters())\n",
    "\n",
    "wm_optimizer = torch.optim.AdamW(wm_params, lr=cfg.model_lr, eps=cfg.epsilon, weight_decay=cfg.weight_decay)\n",
    "actor_optimizer = torch.optim.AdamW(actor.parameters(), lr=cfg.actor_lr, eps=cfg.epsilon, weight_decay=cfg.weight_decay)\n",
    "critic_optimizer = torch.optim.AdamW(critic.parameters(), lr=cfg.critic_lr, eps=cfg.epsilon, weight_decay=cfg.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b872c-8b0f-451b-b8ac-da70edecd2f7",
   "metadata": {
    "id": "fa8b872c-8b0f-451b-b8ac-da70edecd2f7"
   },
   "outputs": [],
   "source": [
    "def evaluation(eval_env: RepeatAction, policy: Agent, step: int, cfg: Config):\n",
    "    \"\"\"\n",
    "    評価用の関数．\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy : Agent\n",
    "        エージェントのインスタンス．\n",
    "    step : int\n",
    "        現状の訓練のステップ数．\n",
    "    cfg : Config\n",
    "        コンフィグ．\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    max_ep_rewards : float\n",
    "        評価中に1エピソードで得た最大の報酬和．\n",
    "    \"\"\"\n",
    "    env = eval_env\n",
    "    all_ep_rewards = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(cfg.eval_episodes):\n",
    "            obs = env.reset()  # 環境をリセット\n",
    "            policy.reset()  # RNNの隠れ状態をリセット\n",
    "            done = False  # 終端条件\n",
    "            episode_reward = 0  # エピソードでの報酬和\n",
    "            while not done:\n",
    "                action = policy(obs, eval=True)\n",
    "                action_int = np.argmax(action)\n",
    "\n",
    "                obs, reward, done, _ = env.step(action_int)\n",
    "                episode_reward += reward\n",
    "\n",
    "            all_ep_rewards.append(episode_reward)\n",
    "\n",
    "        mean_ep_rewards = np.mean(all_ep_rewards)\n",
    "        max_ep_rewards = np.max(all_ep_rewards)\n",
    "        print(f\"Eval(iter={step}) mean: {mean_ep_rewards:.4f} max: {max_ep_rewards:.4f}\")\n",
    "\n",
    "    return max_ep_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a3a0b3-bbe9-4f8f-8ae6-05720a9007b3",
   "metadata": {
    "id": "55a3a0b3-bbe9-4f8f-8ae6-05720a9007b3"
   },
   "outputs": [],
   "source": [
    "# ランダム行動でバッファを埋める\n",
    "obs = env.reset()\n",
    "done = False\n",
    "for _ in range(cfg.seed_iter):\n",
    "    action = env.action_space.sample()\n",
    "    next_obs, reward, done, _ = env.step(action)\n",
    "    action = F.one_hot(torch.tensor(action), num_classes=env.action_space.n)\n",
    "\n",
    "    if done:\n",
    "        replay_buffer.push(preprocess_obs(obs), action, np.tanh(reward), done)\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "\n",
    "    else:\n",
    "        replay_buffer.push(preprocess_obs(obs), action, np.tanh(reward), done)\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b35d8f-5339-42c4-90a3-581d17a3960d",
   "metadata": {
    "id": "a8b35d8f-5339-42c4-90a3-581d17a3960d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 学習を行う\n",
    "# 環境と相互作用 → 一定イテレーションでモデル更新を繰り返す\n",
    "policy = Agent(encoder, rssm, actor)\n",
    "\n",
    "# 環境，収益等の初期化\n",
    "obs = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "total_episode = 1\n",
    "best_reward = -1\n",
    "\n",
    "for iteration in range(NUM_ITER - cfg.seed_iter):\n",
    "    with torch.no_grad():\n",
    "        # 環境と相互作用\n",
    "        action = policy(obs)  # モデルで行動をサンプリング(one-hot)\n",
    "        action_int = np.argmax(action)  # 環境に渡すときはint型\n",
    "        next_obs, reward, done, _ = env.step(action_int)  # 環境を進める\n",
    "\n",
    "        # 得たデータをリプレイバッファに追加して更新\n",
    "        replay_buffer.push(preprocess_obs(obs), action, np.tanh(reward), done)  # x_t, a_t, r_t, gamma_t\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "\n",
    "    if (iteration + 1) % cfg.update_freq == 0:\n",
    "        # モデルの学習\n",
    "        # リプレイバッファからデータをサンプリングする\n",
    "        # (batch size, seq_lenght, *data shape)\n",
    "        observations, actions, rewards, done_flags =\\\n",
    "            replay_buffer.sample(cfg.batch_size, cfg.seq_length)\n",
    "        done_flags = 1 - done_flags  # 終端でない場合に1をとる\n",
    "\n",
    "        # torchで扱える形（seq lengthを最初の次元に，画像はchnnelを最初の次元にする）に変形，observationの前処理\n",
    "        observations = torch.permute(torch.as_tensor(observations, device=device), (1, 0, 4, 2, 3))  # (T, B, C, H, W)\n",
    "        actions = torch.as_tensor(actions, device=device).transpose(0, 1)  # (T, B, action dim)\n",
    "        rewards = torch.as_tensor(rewards, device=device).transpose(0, 1)  # (T, B, 1)\n",
    "        done_flags = torch.as_tensor(done_flags, device=device).transpose(0, 1).float()  # (T, B, 1)\n",
    "\n",
    "        # =================\n",
    "        # world modelの学習\n",
    "        # =================\n",
    "        # 観測をベクトルに埋めこみ\n",
    "        emb_observations = encoder(observations.reshape(-1, 1, 64, 64)).view(cfg.seq_length, cfg.batch_size, -1)  # (T, B, 1536)\n",
    "\n",
    "        # 状態表現z，行動aはゼロで初期化\n",
    "        # バッファから取り出したデータをt={1, ..., seq length}とするなら，以下はz_1とみなせる\n",
    "        state = torch.zeros(cfg.batch_size, cfg.state_dim*cfg.num_classes, device=device)\n",
    "        rnn_hidden = torch.zeros(cfg.batch_size, cfg.rnn_hidden_dim, device=device)\n",
    "\n",
    "        # 各観測に対して状態表現を計算\n",
    "        # タイムステップごとに計算するため，先に格納するTensorを定義する(t={1, ..., seq length})\n",
    "        states = torch.zeros(cfg.seq_length, cfg.batch_size, cfg.state_dim*cfg.num_classes, device=device)\n",
    "        rnn_hiddens = torch.zeros(cfg.seq_length, cfg.batch_size, cfg.rnn_hidden_dim, device=device)\n",
    "\n",
    "        # prior, posteriorを計算してKL lossを計算する\n",
    "        kl_loss = 0\n",
    "        for i in range(cfg.seq_length-1):\n",
    "            # rnn hiddenを更新\n",
    "            rnn_hidden = rssm.recurrent(state, actions[i], rnn_hidden)  # h_t+1\n",
    "\n",
    "            # prior, posteriorを計算\n",
    "            next_state_prior, next_detach_prior = rssm.get_prior(rnn_hidden, detach=True) # \\hat{z}_t+1\n",
    "            next_state_posterior, next_detach_posterior = rssm.get_posterior(rnn_hidden, emb_observations[i+1], detach=True)  # z_t+1\n",
    "\n",
    "            # posteriorからzをサンプリング\n",
    "            state = next_state_posterior.rsample().flatten(1)\n",
    "            rnn_hiddens[i+1] = rnn_hidden  # h_t+1\n",
    "            states[i+1] = state  # z_t+1\n",
    "\n",
    "            # KL lossを計算\n",
    "            kl_loss +=  cfg.kl_balance * torch.mean(kl_divergence(next_detach_posterior, next_state_prior)) + \\\n",
    "                        (1 - cfg.kl_balance) * torch.mean(kl_divergence(next_state_posterior, next_detach_prior))\n",
    "        kl_loss /= (cfg.seq_length - 1)\n",
    "\n",
    "        # 初期状態は使わない\n",
    "        rnn_hiddens = rnn_hiddens[1:]  # (seq lenghth - 1, batch size rnn hidden)\n",
    "        states = states[1:]  # (seq length - 1, batch size, state dim * num_classes)\n",
    "\n",
    "        # 得られた状態を利用して再構成，報酬，終端フラグを予測\n",
    "        # そのままでは時間方向，バッチ方向で次元が多いため平坦化\n",
    "        flatten_rnn_hiddens = rnn_hiddens.view(-1, cfg.rnn_hidden_dim)  # ((T-1) * B, rnn hidden)\n",
    "        flatten_states = states.view(-1, cfg.state_dim * cfg.num_classes)  # ((T-1) * B, state_dim * num_classes)\n",
    "\n",
    "        # 上から再構成，報酬，終端フラグ予測\n",
    "        obs_dist = decoder(flatten_states, flatten_rnn_hiddens)  # (T * B, 3, 64, 64)\n",
    "        reward_dist = reward_model(flatten_states, flatten_rnn_hiddens)  # (T * B, 1)\n",
    "        discount_dist = discount_model(flatten_states, flatten_rnn_hiddens)  # (T * B, 1)\n",
    "\n",
    "        # 各予測に対する損失の計算（対数尤度）\n",
    "        C, H, W = observations.shape[2:]\n",
    "        obs_loss = -torch.mean(obs_dist.log_prob(observations[1:].reshape(-1, C, H, W)))\n",
    "        reward_loss = -torch.mean(reward_dist.log_prob(rewards[:-1].reshape(-1, 1)))\n",
    "        discount_loss = -torch.mean(discount_dist.log_prob(done_flags[:-1].float().reshape(-1, 1)))\n",
    "\n",
    "        # 総和をとってモデルを更新\n",
    "        wm_loss = obs_loss + cfg.reward_loss_scale * reward_loss + cfg.discount_loss_scale * discount_loss + cfg.kl_scale * kl_loss\n",
    "\n",
    "        wm_optimizer.zero_grad()\n",
    "        wm_loss.backward()\n",
    "        clip_grad_norm_(wm_params, cfg.gradient_clipping)\n",
    "        wm_optimizer.step()\n",
    "\n",
    "        #====================\n",
    "        # Actor, Criticの更新\n",
    "        #===================\n",
    "        # wmから得た状態の勾配を切っておく\n",
    "        flatten_rnn_hiddens = flatten_rnn_hiddens.detach()\n",
    "        flatten_states = flatten_states.detach()\n",
    "\n",
    "        # priorを用いた状態予測\n",
    "        # 格納する空のTensorを用意\n",
    "        imagined_states = torch.zeros(cfg.imagination_horizon + 1,\n",
    "                                      *flatten_states.shape,\n",
    "                                      device=flatten_states.device)\n",
    "        imagined_rnn_hiddens = torch.zeros(cfg.imagination_horizon + 1,\n",
    "                                           *flatten_rnn_hiddens.shape,\n",
    "                                           device=flatten_rnn_hiddens.device)\n",
    "        imagined_action_log_probs = torch.zeros((cfg.imagination_horizon, cfg.batch_size * (cfg.seq_length-1)),\n",
    "                                                device=flatten_rnn_hiddens.device)\n",
    "        imagined_action_entropys = torch.zeros((cfg.imagination_horizon, cfg.batch_size * (cfg.seq_length-1)),\n",
    "                                                device=flatten_rnn_hiddens.device)\n",
    "\n",
    "        # 未来予測をして想像上の軌道を作る前に, 最初の状態としては先ほどモデルの更新で使っていた\n",
    "        # リプレイバッファからサンプルされた観測データを取り込んだ上で推論した状態表現を使う\n",
    "        imagined_states[0] = flatten_states\n",
    "        imagined_rnn_hiddens[0] = flatten_rnn_hiddens\n",
    "\n",
    "        # open-loopで予測\n",
    "        for i in range(1, cfg.imagination_horizon + 1):\n",
    "            actions, action_log_probs, action_entropys = actor(flatten_states.detach(), flatten_rnn_hiddens.detach())  # ((T-1) * B, action dim)\n",
    "\n",
    "            # rnn hiddenを更新, priorで次の状態を予測\n",
    "            with torch.no_grad():\n",
    "                flatten_rnn_hiddens = rssm.recurrent(flatten_states, actions, flatten_rnn_hiddens)  # h_t+1\n",
    "                flatten_states_prior = rssm.get_prior(flatten_rnn_hiddens)\n",
    "                flatten_states = flatten_states_prior.rsample().flatten(1)\n",
    "\n",
    "            imagined_rnn_hiddens[i] = flatten_rnn_hiddens.detach()\n",
    "            imagined_states[i] = flatten_states.detach()\n",
    "            imagined_action_log_probs[i-1] = action_log_probs\n",
    "            imagined_action_entropys[i-1] = action_entropys\n",
    "\n",
    "        imagined_states = imagined_states[1:]\n",
    "        imagined_rnn_hiddens = imagined_rnn_hiddens[1:]\n",
    "\n",
    "        # 得られた状態から報酬を予測\n",
    "        flatten_imagined_states = imagined_states.view(-1, cfg.state_dim * cfg.num_classes).detach()  # ((imagination horizon) * (T-1) * B, state dim * num classes)\n",
    "        flatten_imagined_rnn_hiddens = imagined_rnn_hiddens.view(-1, cfg.rnn_hidden_dim).detach()  # ((imagination horizon) * (T-1) * B, rnn hidden)\n",
    "\n",
    "        # reward, done_flagsは分布なので平均値をとる\n",
    "        # ((imagination horizon + 1), (T-1) * B)\n",
    "        with torch.no_grad():\n",
    "            imagined_rewards = reward_model(flatten_imagined_states, flatten_imagined_rnn_hiddens).mean.view(cfg.imagination_horizon, -1)\n",
    "            target_values = target_critic(flatten_imagined_states, flatten_imagined_rnn_hiddens).view(cfg.imagination_horizon, -1)\n",
    "            imagined_done_flags = discount_model(flatten_imagined_states, flatten_imagined_rnn_hiddens).base_dist.probs.view(cfg.imagination_horizon, -1)\n",
    "            discount_arr = cfg.discount * torch.round(imagined_done_flags)\n",
    "\n",
    "        # lambda targetの計算\n",
    "        lambda_target = calculate_lambda_target(imagined_rewards, discount_arr, target_values, cfg.lambda_)\n",
    "\n",
    "        # actorの損失を計算\n",
    "        objective = imagined_action_log_probs * ((lambda_target - target_values).detach())\n",
    "        discount_arr = torch.cat([torch.ones_like(discount_arr[:1]), discount_arr[1:]])\n",
    "        discount = torch.cumprod(discount_arr, 0)\n",
    "        actor_loss = -torch.sum(torch.mean(discount * (objective + cfg.actor_entropy_scale * imagined_action_entropys), dim=1))\n",
    "\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        clip_grad_norm_(actor.parameters(), cfg.gradient_clipping)\n",
    "        actor_optimizer.step()\n",
    "\n",
    "        # criticの損失を計算\n",
    "        value_mean = critic(flatten_imagined_states.detach(), flatten_imagined_rnn_hiddens.detach()).view(cfg.imagination_horizon, -1)\n",
    "        value_dist = td.Independent(td.Normal(value_mean, 1),  1)\n",
    "        critic_loss = -torch.mean(discount.detach() * value_dist.log_prob(lambda_target.detach()).unsqueeze(-1))\n",
    "\n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        clip_grad_norm_(critic.parameters(), cfg.gradient_clipping)\n",
    "        critic_optimizer.step()\n",
    "\n",
    "    if (iteration + 1) % cfg.slow_critic_update == 0:\n",
    "        target_critic.load_state_dict(critic.state_dict())\n",
    "\n",
    "    # エピソードが終了した時に再初期化\n",
    "    if done:\n",
    "        print(f\"episode: {total_episode} total_reward: {total_reward:.8f}\")\n",
    "        print(f\"num iter: {iteration} kl loss: {kl_loss.item():.8f} obs loss: {obs_loss.item():.8f} \"\n",
    "              f\"rewrd loss: {reward_loss.item():.8f} discount loss: {discount_loss.item():.8f} \"\n",
    "              f\"critic loss: {critic_loss.item():.8f} actor loss: {actor_loss.item():.8f}\"\n",
    "        )\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        total_episode += 1\n",
    "        policy.reset()\n",
    "\n",
    "        # 一定エピソードごとに評価\n",
    "        if total_episode % cfg.eval_freq == 0:\n",
    "            eval_reward = evaluation(eval_env, policy, iteration, cfg)\n",
    "            trained_models.save(\"./\")\n",
    "            if eval_reward > best_reward:\n",
    "                best_reward = eval_reward\n",
    "                os.makedirs(\"./best_models\", exist_ok=True)\n",
    "                trained_models.save(\"./best_models\")\n",
    "\n",
    "            eval_env.reset()\n",
    "            policy.reset()\n",
    "\n",
    "trained_models.save(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa693a51-a4cb-4ad4-be2b-322cbd68443d",
   "metadata": {
    "id": "aa693a51-a4cb-4ad4-be2b-322cbd68443d"
   },
   "source": [
    "## 6. モデルの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4373bf-b090-4ab9-9736-05bf9ba84ef3",
   "metadata": {
    "id": "bd4373bf-b090-4ab9-9736-05bf9ba84ef3"
   },
   "outputs": [],
   "source": [
    "# モデルの保存(Google Driveの場合）\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "trained_models.save(\"drive/MyDrive/Colab Notebooks/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b31352-bafa-46ed-8bcc-632a24dfced6",
   "metadata": {
    "id": "c4b31352-bafa-46ed-8bcc-632a24dfced6"
   },
   "source": [
    "## 7. 学習済みパラメータで評価  \n",
    "- こちらの評価に用いている環境は，Omnicampus上で評価する際に用いる環境と同じになっています．\n",
    "- 今回のコンペティションではPublic / Privateの分類はないため，基本的には以下の実装の評価を性能の目安としていただくと良いと思います．  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aa98bd-a9da-4223-9341-0f3cb489211e",
   "metadata": {
    "id": "c7aa98bd-a9da-4223-9341-0f3cb489211e"
   },
   "outputs": [],
   "source": [
    "# 環境の読み込み\n",
    "env = make_env()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 学習済みモデルの読み込み\n",
    "rssm = RSSM(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes, action_dim).to(device)\n",
    "encoder = Encoder().to(device)\n",
    "decoder = Decoder(cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
    "reward_model =  RewardModel(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
    "discount_model = DiscountModel(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
    "actor = Actor(action_dim, cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
    "critic = Critic(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
    "\n",
    "trained_models = TrainedModels(\n",
    "    rssm,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    reward_model,\n",
    "    discount_model,\n",
    "    actor,\n",
    "    critic\n",
    ")\n",
    "\n",
    "trained_models.load(\"./\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a745c7-62dd-4431-99de-4da0f1489a16",
   "metadata": {
    "id": "41a745c7-62dd-4431-99de-4da0f1489a16"
   },
   "outputs": [],
   "source": [
    "# 結果を動画で観てみるための関数\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "def display_video(frames):\n",
    "    plt.figure(figsize=(8, 8), dpi=50)\n",
    "    patch = plt.imshow(frames[0], cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "        plt.title(\"Step %d\" % (i))\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
    "    display(HTML(anim.to_jshtml(default_mode='once')))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e365b2-bc62-4fe9-a9d9-a94e42d382f2",
   "metadata": {
    "id": "12e365b2-bc62-4fe9-a9d9-a94e42d382f2"
   },
   "source": [
    "**環境のシードを固定して評価を行います．シードを変更しないでください．**\n",
    "- 変更した場合，Omnicampus上での評価と結果が異なります．  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206c8746-7371-4142-bd92-dde301c2f33c",
   "metadata": {
    "id": "206c8746-7371-4142-bd92-dde301c2f33c"
   },
   "outputs": [],
   "source": [
    "env = make_env(seed=1234, max_steps=None)\n",
    "\n",
    "policy = Agent(encoder, rssm, actor)\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "frames = [obs]\n",
    "actions = []\n",
    "\n",
    "while not done:\n",
    "    action = policy(obs, eval=True)\n",
    "    action_int = np.argmax(action)  # 環境に渡すときはint型\n",
    "\n",
    "    obs, reward, done, _ = env.step(action_int)\n",
    "\n",
    "    total_reward += reward\n",
    "    frames.append(obs)\n",
    "    actions.append(action_int)\n",
    "\n",
    "print('Total Reward:', total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11353d5f-2551-4d45-9ea0-a1f9ab708dd0",
   "metadata": {
    "id": "11353d5f-2551-4d45-9ea0-a1f9ab708dd0"
   },
   "outputs": [],
   "source": [
    "display_video(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d1b91f-34b5-48ec-9082-0ded1007952c",
   "metadata": {
    "id": "f8d1b91f-34b5-48ec-9082-0ded1007952c"
   },
   "source": [
    "今回，評価を行う際のrepeat actionは1に設定しています．  \n",
    "そのため，repeat actionをそれ以外に設定している場合，repeat actionの分だけ繰り返した行動を提出する形にしています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e850a3d-824a-490a-b12b-9b9ef2100c3d",
   "metadata": {
    "id": "0e850a3d-824a-490a-b12b-9b9ef2100c3d"
   },
   "outputs": [],
   "source": [
    "# repeat actionに対応した行動に変換する\n",
    "submission_actions = np.zeros(len(actions) * env._skip)\n",
    "for start_idx in range(env._skip):\n",
    "    submission_actions[start_idx::env._skip] = np.array(actions)\n",
    "\n",
    "np.save(\"drive/MyDrive/submission\", submission_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebb2707-74f6-43f5-a7cd-99419b3db93d",
   "metadata": {
    "id": "4ebb2707-74f6-43f5-a7cd-99419b3db93d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
